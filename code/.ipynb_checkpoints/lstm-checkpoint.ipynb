{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "d7IxLZotVd4P"
   },
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk import *\n",
    "nltk.download('wordnet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "C9WFlOtW5f7Z"
   },
   "outputs": [],
   "source": [
    "!pip3 install gensim\n",
    "!pip install pulp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "_pXRyM2h1TMc"
   },
   "outputs": [],
   "source": [
    "#kamal\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import math\n",
    "import copy\n",
    "import sys\n",
    "import sklearn.metrics as skm\n",
    "\n",
    "#nalla\n",
    "from itertools import product\n",
    "from collections import defaultdict\n",
    "import csv\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "import gensim\n",
    "# from pyemd import emd\n",
    "from gensim import corpora, models, similarities\n",
    "from scipy.spatial.distance import euclidean\n",
    "import pulp\n",
    "\n",
    "#sowrya\n",
    "from nltk import word_tokenize, pos_tag\n",
    "from nltk.corpus import wordnet as wn\n",
    "from six import iteritems\n",
    "from collections import deque"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "ebjCak-E5-iR"
   },
   "outputs": [],
   "source": [
    "from google.colab import drive\n",
    "drive.mount('/content/drive')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "5HoRO6sn1TMU"
   },
   "source": [
    "# Paraphrase Detection on Twitter"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "7JH9TLQt1TM3"
   },
   "source": [
    "## Reading data\n",
    "* each tweet is considered as a document\n",
    "* making a list of all the tweets and finding the idf values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "AVo5hrCF1TM8"
   },
   "outputs": [],
   "source": [
    "def glove_processing():\n",
    "    f = open('./drive/My Drive/glove/glove25.txt','r')\n",
    "    model = {}\n",
    "    for line in f:\n",
    "        line = line.split()\n",
    "        word = line[0]\n",
    "        embedding = np.array([float(val) for val in line[1:]])\n",
    "        model[word] =  embedding\n",
    "    print(len(model))\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "X91RpZaz1TNI"
   },
   "outputs": [],
   "source": [
    "df = pd.read_csv('./drive/My Drive/Data/data/train.data', sep='\\t', lineterminator='\\n', names=['Topic_Id', 'Topic_Name',  'Sent_1', 'Sent_2', 'Label', 'Sent_1_tag','Sent_2_tag'])\n",
    "df2 = df[['Sent_1', 'Sent_2', 'Label']]\n",
    "model = glove_processing()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "3JexbBzUNwc7"
   },
   "outputs": [],
   "source": [
    "print(df2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "mKN1S9qv1TNa"
   },
   "source": [
    "## Preprocessing Tweets\n",
    "\n",
    "* Preprocess the tweets with respect to .... to get a 4D vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "nvAs4xOp1TNc"
   },
   "outputs": [],
   "source": [
    "def process_labels(labels):\n",
    "    '''returns binary labels 0 or 1'''\n",
    "    new_labels = np.array([])\n",
    "    for i in labels:\n",
    "        if type(i) == str and len(i) > 1:\n",
    "            i = i[1]\n",
    "        if int(i) >= 3:\n",
    "            new_labels = np.append(new_labels, int(1))\n",
    "        else:\n",
    "            new_labels = np.append(new_labels, int(0))\n",
    "    return new_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "6aK7snmY1TNn"
   },
   "outputs": [],
   "source": [
    "train_labels = process_labels(df2.Label)\n",
    "print(train_labels.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "AJBU1xry1TN4"
   },
   "source": [
    "### Cosine Distance for average of sentence vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "YRmDXnvG1TN8"
   },
   "outputs": [],
   "source": [
    "def sent_vec(sent, model):\n",
    "    ans = np.zeros(25)\n",
    "    count = 0\n",
    "    if len(sent) == 0:\n",
    "        return ans\n",
    "    for i, word in enumerate(sent.split()):\n",
    "        if word not in model:\n",
    "            continue\n",
    "        else:\n",
    "            ans += np.array(model[word])\n",
    "            count += 1\n",
    "    if count > 0:\n",
    "        ans = ans / count\n",
    "    return ans, count\n",
    "\n",
    "\n",
    "def cosine_dist(vec1, vec2):\n",
    "    ans = 0\n",
    "    mod1 = 0\n",
    "    mod2 = 0\n",
    "    for i in range(vec1.size):\n",
    "        ans += vec1[i] * vec2[i]\n",
    "        mod1 += vec1[i] * vec1[i]\n",
    "        mod2 += vec2[i] * vec2[i]\n",
    "    if ans == 0:\n",
    "        return 0\n",
    "    return ans / (math.sqrt(mod1) * math.sqrt(mod2))\n",
    "\n",
    "\n",
    "def cosine_data(df, model):\n",
    "    arr = np.array([]) #has the cosine distance values for all the tweet pairs\n",
    "    for i in range(df.Sent_1.size):\n",
    "        vec1, count1 = sent_vec(df.Sent_1[i], model)\n",
    "        vec2, count2 = sent_vec(df.Sent_2[i], model)\n",
    "        ans = cosine_dist(vec1, vec2)\n",
    "        arr = np.append(arr, ans)\n",
    "#         if i == 7:\n",
    "#             print(i, df.Sent_1[i], \"sdfsdf\", df.Sent_2[i], vec1, vec2, count1, count2, ans)\n",
    "    return arr"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "iSrIOfWe1TOK"
   },
   "source": [
    "### Word movers distance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 52
    },
    "colab_type": "code",
    "id": "2s1BBiZd1TOM",
    "outputId": "098e335d-8d8b-4d03-c2a1-8a66d992ed67"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('stopwords')\n",
    "singleindexing = lambda m, i, j: m*i+j\n",
    "unpackindexing = lambda m, k: (k/m, k % m)\n",
    "\n",
    "def tokens_to_fracdict(tokens):\n",
    "    cntdict = defaultdict(lambda : 0)\n",
    "    for token in tokens:\n",
    "        cntdict[token] += 1\n",
    "    totalcnt = sum(cntdict.values())\n",
    "    return {token: float(cnt)/totalcnt for token, cnt in cntdict.items()}\n",
    "\n",
    "# use PuLP\n",
    "def word_mover_distance_probspec(first_sent_tokens, second_sent_tokens, wvmodel, lpFile=None):\n",
    "    all_tokens = list(set(first_sent_tokens+second_sent_tokens))\n",
    "    wordvecs = {}\n",
    "    for token in all_tokens:\n",
    "        try:\n",
    "            wordvecs[token] = wvmodel[token]\n",
    "        except KeyError:\n",
    "            wordvecs[token] = 0\n",
    "\n",
    "    first_sent_buckets = tokens_to_fracdict(first_sent_tokens)\n",
    "    second_sent_buckets = tokens_to_fracdict(second_sent_tokens)\n",
    "\n",
    "    T= pulp.LpVariable.dicts('T_matrix', list(product(all_tokens, all_tokens)), lowBound=0)\n",
    "\n",
    "    prob = pulp.LpProblem('WMD', sense=pulp.LpMinimize)\n",
    "    try:\n",
    "        prob += pulp.lpSum([T[token1, token2]*euclidean(wordvecs[token1], wordvecs[token2])\n",
    "                        for token1, token2 in product(all_tokens, all_tokens)])\n",
    "    except KeyError:\n",
    "        prob+=0\n",
    "    for token2 in second_sent_buckets:\n",
    "        prob += pulp.lpSum([T[token1, token2] for token1 in first_sent_buckets])==second_sent_buckets[token2]\n",
    "    for token1 in first_sent_buckets:\n",
    "        prob += pulp.lpSum([T[token1, token2] for token2 in second_sent_buckets])==first_sent_buckets[token1]\n",
    "\n",
    "    if lpFile!=None:\n",
    "        prob.writeLP(lpFile)\n",
    "    prob.solve()\n",
    "    if(prob == None):\n",
    "        print(\"yes\")\n",
    "    return prob\n",
    "\n",
    "def word_mover_distance(first_sent_tokens, second_sent_tokens, wvmodel, lpFile=None):\n",
    "    prob = word_mover_distance_probspec(first_sent_tokens, second_sent_tokens, wvmodel, lpFile=lpFile)\n",
    "    return pulp.value(prob.objective)\n",
    "\n",
    "def arr2_creator(df, model):\n",
    "    arr2 = np.array([])\n",
    "    stop_words = stopwords.words('english')\n",
    "    dist=0\n",
    "    l = open('./drive/My Drive/Data/data/train.data','r')\n",
    "    j = 0\n",
    "    total = 0\n",
    "    for i in range(df.Sent_1.size):\n",
    "        sent1 = df.Sent_1[i].split()\n",
    "        sent2 = df.Sent_2[i].split()\n",
    "        sent1 = [w for w in sent1 if w not in stop_words]\n",
    "        sent2 = [w for w in sent2 if w not in stop_words]\n",
    "        temp = word_mover_distance(sent1, sent2, model)\n",
    "        if temp == None:\n",
    "            temp = total / j\n",
    "        else:\n",
    "            total += temp\n",
    "            j += 1\n",
    "        val = 0\n",
    "        if temp == 0:\n",
    "            val = 2\n",
    "        else:\n",
    "            val = 1/temp\n",
    "        arr2 = np.append(arr2, val)\n",
    "    return arr2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "cR4eG4BJ1TOX"
   },
   "source": [
    "### Wordnet based noun similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 121
    },
    "colab_type": "code",
    "id": "Gp1zHemB1TOY",
    "outputId": "588c71c6-ca41-4cc4-e763-2686f0d5acae"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     /root/nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n",
      "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('averaged_perceptron_tagger')\n",
    "nltk.download('punkt')\n",
    "def penn_to_wn(tag):\n",
    "    if tag.startswith('N'):\n",
    "        return 'n'\n",
    "    # if tag.startswith('V'):\n",
    "    #     return 'v'\n",
    "    # if tag.startswith('J'):\n",
    "    #     return 'a'\n",
    "    # if tag.startswith('R'):\n",
    "    #     return 'r'\n",
    "    return None\n",
    "\n",
    "def tagged_to_synset(word, tag):\n",
    "    wn_tag = penn_to_wn(tag)\n",
    "    if wn_tag is None:\n",
    "#         print('here')\n",
    "        return None\n",
    "    try:\n",
    "        return wn.synsets(word, wn_tag)[0]\n",
    "        print('here')\n",
    "    except:\n",
    "#         print(wn.synsets(word, wn_tag))\n",
    "#         print('here_1')\n",
    "        return None\n",
    "\n",
    "def shortest_hypernym_paths(synset):\n",
    "    # hyp = synset.hypernyms()\n",
    "    # hyp_1 = synset.instance_hypernyms()\n",
    "    if synset._name == '*ROOT*':\n",
    "        return {synset: 0}\n",
    "    queue = deque([(synset,0)])\n",
    "    path = {}\n",
    "    while queue:\n",
    "        s, depth = queue.popleft()\n",
    "        if s in path:\n",
    "            continue\n",
    "        path[s] = depth\n",
    "        depth += 1\n",
    "        queue.extend((hyp, depth) for hyp in s.hypernyms())\n",
    "        queue.extend((hyp, depth) for hyp in s.instance_hypernyms())\n",
    "    # simulate_root = True\n",
    "    # # if simulate_root:\n",
    "    # #     fake_synset = wn.synset(None)\n",
    "    # #     fake_synset._name = '*ROOT*'\n",
    "    # #     path[fake_synset] = max(path.values()) + 1\n",
    "    return path\n",
    "\n",
    "def shortest_path_distance(synset_1,synset_2):\n",
    "    if synset_1 == synset_2:\n",
    "        return 0\n",
    "    dist_dict1 = shortest_hypernym_paths(synset_1)\n",
    "    dist_dict2 = shortest_hypernym_paths(synset_2)\n",
    "    inf = float('inf')\n",
    "    path_distance = inf\n",
    "    for synset, d1 in iteritems(dist_dict1):\n",
    "        d2 = dist_dict2.get(synset, inf)\n",
    "        path_distance = min(path_distance, d1 + d2)\n",
    "    return None if math.isinf(path_distance) else path_distance\n",
    "\n",
    "def path_similarity(synset_1,synset_2):\n",
    "    distance = shortest_path_distance(synset_1,synset_2)\n",
    "    if distance is None or distance < 0:\n",
    "        return None\n",
    "    return 1.0/(distance + 1)\n",
    "\n",
    "def sentence_similarity(sentence1, sentence2):\n",
    "    sentence1, sentence2 = pos_tag(word_tokenize(sentence1)), pos_tag(word_tokenize(sentence2))\n",
    "    synsets1 = [tagged_to_synset(*tagged_word) for tagged_word in sentence1]\n",
    "    synsets1 = [ss for ss in synsets1 if ss]\n",
    "    synsets2 = [tagged_to_synset(*tagged_word) for tagged_word in sentence2]\n",
    "    synsets2 = [ss for ss in synsets2 if ss]\n",
    "    score, count = 0.0, 0\n",
    "    for syn1 in synsets1:\n",
    "        arr_simi_score = []\n",
    "        for syn2 in synsets2:\n",
    "            simi_score = path_similarity(syn1,syn2)\n",
    "            if simi_score is not None:\n",
    "                arr_simi_score.append(simi_score)\n",
    "        if(len(arr_simi_score) > 0):\n",
    "            best = max(arr_simi_score)\n",
    "            score += best\n",
    "            count += 1\n",
    "    if count == 0:\n",
    "        score = 0\n",
    "    else:\n",
    "        score /= count\n",
    "    return score\n",
    "\n",
    "def arr3_creator(df):\n",
    "    arr3 = np.array([])\n",
    "    for i in range(df.Sent_1.size):\n",
    "        val = (sentence_similarity(df.Sent_1[i], df.Sent_2[i]) + sentence_similarity(df.Sent_2[i], df.Sent_1[i]))/2\n",
    "        arr3 = np.append(arr3, val)\n",
    "    return arr3\n",
    "  \n",
    "  \n",
    "# def arr3_creator(df):\n",
    "#     arr3 = np.array([])\n",
    "#     s_1 = \"EJ Manuel the 1st QB to go in this draft\"\n",
    "#     s_2 = \"if EJ is the 1st QB off the board\"\n",
    "#     for i in range(df.Sent_1.size):\n",
    "# #     val = (sentence_similarity(s_2, s_1) + sentence_similarity(s_1, s_2))/2\n",
    "#         val = (sentence_similarity(df.Sent_1[i], df.Sent_2[i]) + sentence_similarity(df.Sent_2[i], df.Sent_1[i]))/2\n",
    "# #         print(df.Sent_1[i], df.Sent_2[i], val)\n",
    "#         arr3 = np.append(arr3, val)\n",
    "#     return arr3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "jCyfAy5h1TOe"
   },
   "source": [
    "### Creating vectors\n",
    "* Appending all values for each tweet pair to create a 4D vector "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "tFHe9rDC1TOf",
    "outputId": "575d0e8b-418b-4f51-a545-0d5c95faf589"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(13063,)\n"
     ]
    }
   ],
   "source": [
    "arr1 = cosine_data(df2, model)\n",
    "print(arr1.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "aVSPXpbF1TOp",
    "outputId": "9220b585-76d2-4472-cdc6-fc62f1429e44"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(13063,)\n"
     ]
    }
   ],
   "source": [
    "arr2 = arr2_creator(df2, model)\n",
    "print(arr2.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "5RMYijO01TOz",
    "outputId": "5f24de72-4320-4ae5-dc6e-40923cda21ed"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(13063,)\n"
     ]
    }
   ],
   "source": [
    "arr3 = arr3_creator(df2)\n",
    "print(arr3.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "ksSmhjKm1TPB"
   },
   "outputs": [],
   "source": [
    "print(arr1[:100])\n",
    "print(arr2[:100])\n",
    "print(arr3[:100])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "umEKT4cB1TPJ"
   },
   "outputs": [],
   "source": [
    "def create_vectors(arr1, arr2, arr3):\n",
    "    new_arr = np.vstack((arr1, arr2, arr3))\n",
    "    return new_arr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "MsD7yB9h1TPO",
    "outputId": "d227a148-60a1-4081-f6a4-2f3e0ef51ef5"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(3, 13063)\n"
     ]
    }
   ],
   "source": [
    "train_data = create_vectors(arr1, arr2, arr3) #(3,N) dimensions\n",
    "print(train_data.shape)\n",
    "#each column is a vector"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "H-vnutcm1TPX"
   },
   "source": [
    "## Training Models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "69IG5LxX1TPZ"
   },
   "source": [
    "### Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Hrj1Dw5q1TPb"
   },
   "outputs": [],
   "source": [
    "def loss(model, train_data, train_labels):\n",
    "    ans = 0\n",
    "    for i in range(train_labels.size):\n",
    "#         print(np.dot(model, train_data[:, i]))\n",
    "        val = sigmoid(np.dot(model, train_data[:, i]))\n",
    "        ans += (train_labels[i] * np.log(val)) + ((1-train_labels[i]) * np.log(1-val))\n",
    "    return abs(ans)\n",
    "\n",
    "def classify(val):\n",
    "    if val >= 0.5:\n",
    "        return 1\n",
    "    else:\n",
    "        return 0\n",
    "\n",
    "def sigmoid(val):\n",
    "    ans = 1 / (1 + np.exp(-val))\n",
    "    return ans\n",
    "\n",
    "def loss_gradient(model, train_data, train_labels):\n",
    "    loss = np.zeros(model.size)\n",
    "    for j in range(train_labels.size):\n",
    "#         print(j, train_data[:, j])\n",
    "        loss = loss + train_data[:, j] * (sigmoid(np.dot(model, train_data[:, j])) - train_labels[j])\n",
    "#         if loss[1] == None:\n",
    "#             print (j, train_data[:,j])\n",
    "    return loss\n",
    "\n",
    "def log_reg(train_data, train_labels):\n",
    "    '''returns a trained model'''\n",
    "    eta = 0.00005\n",
    "    model = np.random.rand(3)\n",
    "    curr_loss = loss(model, train_data, train_labels)\n",
    "    prev_loss = curr_loss - 100\n",
    "    while abs(curr_loss - prev_loss) > 1:\n",
    "        print(abs(curr_loss - prev_loss))\n",
    "#     while loss(model, train_data, train_labels) > 0.1:\n",
    "        val = loss_gradient(model, train_data, train_labels)\n",
    "        model -= eta * val\n",
    "        prev_loss = curr_loss\n",
    "        curr_loss = loss(model, train_data, train_labels)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "v_Si24j01TPg"
   },
   "outputs": [],
   "source": [
    "lr_model = log_reg(train_data, train_labels)\n",
    "# print(lr_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "upA5P9_K1TPn",
    "outputId": "56fa3f23-d1ef-4554-b208-393027ca37ad"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[-1.42810655  0.67765735  0.29323318]\n"
     ]
    }
   ],
   "source": [
    "print(lr_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "NpiD_lF61TPu"
   },
   "source": [
    "# testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "VwAPmgiX1TPw"
   },
   "outputs": [],
   "source": [
    "df = pd.read_csv('./drive/My Drive/Data/data/test.data', sep='\\t', lineterminator='\\n', names=['Topic_Id', 'Topic_Name',  'Sent_1', 'Sent_2', 'Label', 'Sent_1_tag','Sent_2_tag'])\n",
    "test_pairs = df[['Sent_1', 'Sent_2']]\n",
    "# print(np.shape(test_pairs))\n",
    "test_labels = process_labels(df.Label)\n",
    "print(df.Sent_1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "BQ8_HKCA1TP0",
    "outputId": "782afa9f-9702-496e-9dfc-9a83fcfd7f3d"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(972,)\n"
     ]
    }
   ],
   "source": [
    "test_arr1 = cosine_data(test_pairs, model)\n",
    "print(test_arr1.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "gMjcay-f1TP7",
    "outputId": "61050a7d-2b54-4deb-9672-71e02298ea0b"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(972,)\n"
     ]
    }
   ],
   "source": [
    "test_arr2 = arr2_creator(test_pairs, model)\n",
    "print(test_arr2.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "M-PRZgbe1TQJ",
    "outputId": "6824c460-7750-4954-fd1c-114dabb3bb74"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(972,)\n"
     ]
    }
   ],
   "source": [
    "test_arr3 = arr3_creator(test_pairs)\n",
    "print(test_arr3.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "oTnC5sKT1TQU"
   },
   "outputs": [],
   "source": [
    "test_data = create_vectors(test_arr1, test_arr2, test_arr3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "0ayw2mXP1TQY"
   },
   "outputs": [],
   "source": [
    "def testing(model, test_data, test_labels):\n",
    "    predict = np.array([])\n",
    "    for i in range(test_labels.size):\n",
    "        val = classify(sigmoid(np.dot(model, test_data[:, i])))\n",
    "        predict = np.append(predict, val)\n",
    "    return predict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "PFgu-ikf1TQb"
   },
   "outputs": [],
   "source": [
    "predicted_arr = testing(lr_model, test_data, test_labels)\n",
    "print(skm.classification_report(test_labels, predicted_arr))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "KPcQ_i2i1TQh"
   },
   "outputs": [],
   "source": [
    "print(skm.accuracy_score(test_labels, predicted_arr))\n",
    "# print(train_data[2])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "L5hKX2WN1TQn"
   },
   "source": [
    "## MLP\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "JuQXj9fp1TQp"
   },
   "outputs": [],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.utils import to_categorical\n",
    "from keras.utils import np_utils\n",
    "from keras.layers.core import Dense, Activation, Dropout"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "kgtE2B551TQu"
   },
   "outputs": [],
   "source": [
    "def MLP(train_data, train_labels, test_data, test_labels):\n",
    "    X_train = train_data.T\n",
    "    y_train = train_labels\n",
    "    y_binary = to_categorical(y_train)\n",
    "\n",
    "    scale = np.max(X_train)\n",
    "    X_train /= scale\n",
    "\n",
    "    mean = np.std(X_train)\n",
    "    X_train -= mean\n",
    "\n",
    "    input_dim = X_train.shape[1]\n",
    "#     print?(X_train[0])\n",
    "#     print(input_dim)\n",
    "    nb_classes = 1\n",
    "    \n",
    "    model = Sequential()\n",
    "    model.add(Dense(128, input_dim=input_dim))\n",
    "    model.add(Activation('relu'))\n",
    "    model.add(Dropout(0.2))\n",
    "    model.add(Dense(128))\n",
    "    model.add(Activation('relu'))\n",
    "    model.add(Dropout(0.2))\n",
    "    model.add(Dense(nb_classes))\n",
    "    model.add(Activation('sigmoid'))\n",
    "#     model = Sequential()\n",
    "#     model.add(Dense(64, input_dim=input_dim, activation='relu'))\n",
    "#     model.add(Dropout(0.5))\n",
    "#     model.add(Dense(64, activation='relu'))\n",
    "#     model.add(Dropout(0.5))\n",
    "#     model.add(Dense(1, activation='sigmoid'))\n",
    "\n",
    "    # we'll use categorical xent for the loss, and RMSprop as the optimizer\n",
    "    model.compile(loss='mse', optimizer='rmsprop')\n",
    "\n",
    "    print(\"Training...\")\n",
    "    model.fit(X_train, y_train, nb_epoch=50, batch_size=128, verbose=2)\n",
    "    \n",
    "    X_test = test_data.T\n",
    "    y_test = test_labels\n",
    "    preds = model.predict_classes(X_test, verbose=0)\n",
    "\n",
    "    score = model.evaluate(X_test, y_test, batch_size=128)\n",
    "    print(\"score\",score)\n",
    "    return preds, model\n",
    "# print(\"Generating test predictions...\")\n",
    "# preds = model.predict_classes(X_test, verbose=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "8RY5rimM0UGO"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "pzK3sf9e1TQ1",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "preds = np.array([])\n",
    "preds, mlp_model = MLP(train_data, train_labels, test_data, test_labels)\n",
    "# print(mlp_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "kKN24_DB1TQ-"
   },
   "outputs": [],
   "source": [
    "# predicted_arr = testing(mlp_model, test_data, test_labels)\n",
    "print(skm.classification_report(test_labels, preds))\n",
    "print(skm.accuracy_score(test_labels, preds))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "wtm4G0hZ1TRD"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "PO9yMc6t1TRH"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "J9xRbK6y1TRK"
   },
   "outputs": [],
   "source": [
    "from sklearn.externals import joblib\n",
    "joblib.dump(model, \"./drive/My Drive/Data/\" + (str)model+ \".joblib\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "X8Uv7CAfVXsN"
   },
   "outputs": [],
   "source": [
    "print(df2.Sent_1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "dPQs9OTA7JCZ"
   },
   "source": [
    "**RNN**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Oa8nlkTL7Hua"
   },
   "outputs": [],
   "source": [
    "from time import time\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from gensim.models import KeyedVectors\n",
    "import re\n",
    "from nltk.corpus import stopwords\n",
    "from sklearn.model_selection import train_test_split\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "import itertools\n",
    "import datetime\n",
    "\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.models import Sequential, Model\n",
    "from keras.layers import Input, Embedding, LSTM, Lambda, Dense,concatenate\n",
    "import keras.backend as K\n",
    "from keras.optimizers import Adadelta\n",
    "from keras.callbacks import ModelCheckpoint\n",
    "from keras.layers.normalization import BatchNormalization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "vdAGA5kJ7R8N"
   },
   "outputs": [],
   "source": [
    "TRAIN_CSV = './drive/My Drive/Data/data/train.data'\n",
    "TEST_CSV = './drive/My Drive/Data/data/test.data'\n",
    "EMBEDDING_FILE = './drive/My Drive/glove/glove25.txt'\n",
    "MODEL_SAVING_DIR = './drive/My Drive/Data/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 52
    },
    "colab_type": "code",
    "id": "r1ogEJZu7hEf",
    "outputId": "2c1830ae-f287-471f-c5b1-2a8e58678164"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "nltk.download('stopwords')\n",
    "stops = set(stopwords.words('english'))\n",
    "\n",
    "def text_to_word_list(text):\n",
    "    ''' Pre process and convert texts to a list of words '''\n",
    "    text = str(text)\n",
    "    text = text.lower()\n",
    "\n",
    "    # Clean the text\n",
    "    text = re.sub(r\"[^A-Za-z0-9^,!.\\/'+-=]\", \" \", text)\n",
    "    text = re.sub(r\"what's\", \"what is \", text)\n",
    "    text = re.sub(r\"\\'s\", \" \", text)\n",
    "    text = re.sub(r\"\\'ve\", \" have \", text)\n",
    "    text = re.sub(r\"can't\", \"cannot \", text)\n",
    "    text = re.sub(r\"n't\", \" not \", text)\n",
    "    text = re.sub(r\"i'm\", \"i am \", text)\n",
    "    text = re.sub(r\"\\'re\", \" are \", text)\n",
    "    text = re.sub(r\"\\'d\", \" would \", text)\n",
    "    text = re.sub(r\"\\'ll\", \" will \", text)\n",
    "    text = re.sub(r\",\", \" \", text)\n",
    "    text = re.sub(r\"\\.\", \" \", text)\n",
    "    text = re.sub(r\"!\", \" ! \", text)\n",
    "    text = re.sub(r\"\\/\", \" \", text)\n",
    "    text = re.sub(r\"\\^\", \" ^ \", text)\n",
    "    text = re.sub(r\"\\+\", \" + \", text)\n",
    "    text = re.sub(r\"\\-\", \" - \", text)\n",
    "    text = re.sub(r\"\\=\", \" = \", text)\n",
    "    text = re.sub(r\"'\", \" \", text)\n",
    "    text = re.sub(r\"(\\d+)(k)\", r\"\\g<1>000\", text)\n",
    "    text = re.sub(r\":\", \" : \", text)\n",
    "    text = re.sub(r\" e g \", \" eg \", text)\n",
    "    text = re.sub(r\" b g \", \" bg \", text)\n",
    "    text = re.sub(r\" u s \", \" american \", text)\n",
    "    text = re.sub(r\"\\0s\", \"0\", text)\n",
    "    text = re.sub(r\" 9 11 \", \"911\", text)\n",
    "    text = re.sub(r\"e - mail\", \"email\", text)\n",
    "    text = re.sub(r\"j k\", \"jk\", text)\n",
    "    text = re.sub(r\"\\s{2,}\", \" \", text)\n",
    "\n",
    "    text = text.split()\n",
    "\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "G0HbiOjq7kyZ"
   },
   "outputs": [],
   "source": [
    "def prep_embs(train_df, test_df, word2vec):\n",
    "    vocabulary = dict()\n",
    "    inverse_vocabulary = ['<unk>']  # '<unk>' will never be used, it is only a placeholder for the [0, 0, ....0] embedding\n",
    "    # word2vec = KeyedVectors.load_word2vec_format(EMBEDDING_FILE, binary=True)\n",
    "\n",
    "    questions_cols = ['Sent_1', 'Sent_2']\n",
    "\n",
    "    # Iterate over the questions only of both training and test datasets\n",
    "    for dataset in [train_df, test_df]:\n",
    "        for index, row in dataset.iterrows():\n",
    "            for question in questions_cols:\n",
    "                q2n = []  # q2n -> question numbers representation\n",
    "                for word in text_to_word_list(row[question]):\n",
    "\n",
    "                    # Check for unwanted words\n",
    "                    if word in stops and word not in word2vec:\n",
    "                        continue\n",
    "\n",
    "                    if word not in vocabulary:\n",
    "                        vocabulary[word] = len(inverse_vocabulary)\n",
    "                        q2n.append(len(inverse_vocabulary))\n",
    "                        inverse_vocabulary.append(word)\n",
    "                    else:\n",
    "                        q2n.append(vocabulary[word])\n",
    "\n",
    "                # Replace questions as word to question as number representation\n",
    "                dataset.set_value(index, question, q2n)\n",
    "\n",
    "    embedding_dim = 25\n",
    "    embeddings = 1 * np.random.randn(len(vocabulary) + 1, embedding_dim)  # This will be the embedding matrix\n",
    "    embeddings[0] = 0  # So that the padding will be ignored\n",
    "\n",
    "    # Build the embedding matrix\n",
    "    for word, index in vocabulary.items():\n",
    "        if word in word2vec:\n",
    "            embeddings[index] = word2vec[word]\n",
    "    return embeddings, embedding_dim\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 72
    },
    "colab_type": "code",
    "id": "JV8pvcNd7p-o",
    "outputId": "21b85da2-9a10-461b-d13a-59cc41fb1191"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:27: FutureWarning: set_value is deprecated and will be removed in a future release. Please use .at[] or .iat[] accessors instead\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9515\n"
     ]
    }
   ],
   "source": [
    "embeddings, embedding_dim = prep_embs(df2, df, model)\n",
    "print(len(embeddings))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "16m1GezvS3y8"
   },
   "outputs": [],
   "source": [
    "max_seq_length = max(df2.Sent_1.map(lambda x: len(x)).max(),\n",
    "                     df2.Sent_2.map(lambda x: len(x)).max(),\n",
    "                     test_pairs.Sent_1.map(lambda x: len(x)).max(),\n",
    "                     test_pairs.Sent_2.map(lambda x: len(x)).max())\n",
    "\n",
    "# print(max_seq_length)\n",
    "# Split to train validation\n",
    "validation_size = 0.1\n",
    "# training_size = len(df2) - validation_size\n",
    "\n",
    "questions_cols = ['Sent_1', 'Sent_2']\n",
    "\n",
    "X = df2[questions_cols]\n",
    "Y = train_labels\n",
    "\n",
    "X_train, X_validation, Y_train, Y_validation = train_test_split(X, Y, test_size=validation_size)\n",
    "X1 = X_train\n",
    "# print(np.shape(Y_train))\n",
    "# Split to dicts\n",
    "X_train = {'left': X1.Sent_1, 'right': X1.Sent_2}\n",
    "X_validation = {'left': X_validation.Sent_1, 'right': X_validation.Sent_2}\n",
    "X_test = {'left': df.Sent_1, 'right': df.Sent_2}\n",
    "# print(np.shape(X_train['left']))\n",
    "# print(df.Sent_1)\n",
    "i = 0\n",
    "for dataset, side in itertools.product([X_train, X_validation, X_test], ['left', 'right']):\n",
    "    dataset[side] = pad_sequences(dataset[side], maxlen=max_seq_length)\n",
    "#     print(np.size(dataset[side]))\n",
    "#     print(np.shape(X_train['left']))\n",
    "\n",
    "\n",
    "# Make sure everything is ok\n",
    "assert X_train['left'].shape == X_train['right'].shape\n",
    "assert len(X_train['left']) == len(Y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 52
    },
    "colab_type": "code",
    "id": "3YAZCdQKTrgw",
    "outputId": "5b7e31df-ce68-412e-a0c1-85bb22429a34"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(11756, 101)\n",
      "(972, 101)\n"
     ]
    }
   ],
   "source": [
    "print(np.shape(X_train['left']))\n",
    "print(np.shape(X_test['left']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "2sWae0p-TiXv"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "kaG2zAAgTEOw"
   },
   "outputs": [],
   "source": [
    "n_hidden = 50\n",
    "gradient_clipping_norm = 1.25\n",
    "batch_size = 64\n",
    "n_epoch = 50\n",
    "def exponent_neg_manhattan_distance(left, right):\n",
    "    ''' Helper function for the similarity estimate of the LSTMs outputs'''\n",
    "    return K.exp(-K.sum(K.abs(left-right), axis=1, keepdims=True))\n",
    "\n",
    "# The visible layer\n",
    "left_input = Input(shape=(max_seq_length,), dtype='int32')\n",
    "right_input = Input(shape=(max_seq_length,), dtype='int32')\n",
    "# l = Sequential()\n",
    "embedding_layer = Embedding(len(embeddings), embedding_dim, weights=[embeddings], input_length=max_seq_length, trainable=False)\n",
    "\n",
    "# Embedded version of the inputs\n",
    "encoded_left = embedding_layer(left_input)\n",
    "encoded_right = embedding_layer(right_input)\n",
    "\n",
    "# Since this is a siamese network, both sides share the same LSTM\n",
    "shared_lstm = LSTM(n_hidden)\n",
    "\n",
    "left_output = shared_lstm(encoded_left)\n",
    "right_output = shared_lstm(encoded_right)\n",
    "\n",
    "# Calculates the distance as defined by the MaLSTM model\n",
    "merged_vector = concatenate([left_output, right_output], axis=-1)\n",
    "malstm_distance = Lambda(function=lambda x: exponent_neg_manhattan_distance(x[0], x[1]),output_shape=lambda x: (x[0][0], 1))([left_output, right_output])\n",
    "\n",
    "# malstm_distance = BatchNormalization()(malstm_distance)\n",
    "out = Dense(1, activation='sigmoid')(malstm_distance)\n",
    "# # Pack it all up into a model\n",
    "malstm = Model([left_input, right_input],outputs=out)\n",
    "# malstm.add(Dense(1, activation='sigmoid'))\n",
    "# Adadelta optimizer, with gradient clipping by norm\n",
    "optimizer = Adadelta(clipnorm=gradient_clipping_norm)\n",
    "\n",
    "# malstm.compile(loss='mean_squared_error', optimizer=optimizer, metrics=['accuracy'])\n",
    "malstm.compile(optimizer=optimizer, loss='binary_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# Start training\n",
    "training_start_time = time()\n",
    "\n",
    "malstm_trained = malstm.fit([X_train['left'], X_train['right']], Y_train, batch_size=batch_size, nb_epoch=n_epoch,\n",
    "                            validation_data=([X_validation['left'], X_validation['right']], Y_validation))\n",
    "\n",
    "print(\"Training time finished.\\n{} epochs in {}\".format(n_epoch, datetime.timedelta(seconds=time()-training_start_time)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "askss2NW8VUL"
   },
   "outputs": [],
   "source": [
    "# Plot accuracy\n",
    "plt.plot(malstm_trained.history['acc'])\n",
    "plt.plot(malstm_trained.history['val_acc'])\n",
    "plt.title('Model Accuracy')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.xlabel('Epoch')\n",
    "plt.legend(['Train', 'Validation'], loc='upper left')\n",
    "plt.show()\n",
    "\n",
    "# Plot loss\n",
    "plt.plot(malstm_trained.history['loss'])\n",
    "plt.plot(malstm_trained.history['val_loss'])\n",
    "plt.title('Model Loss')\n",
    "plt.ylabel('Loss')\n",
    "plt.xlabel('Epoch')\n",
    "plt.legend(['Train', 'Validation'], loc='upper right')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "ov6Lw7Lt-DNr",
    "outputId": "9baf21cb-8262-4847-a670-7c8aad3b4888"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.6882716049382716\n"
     ]
    }
   ],
   "source": [
    "# print(X_test['input_1'])\n",
    "# X_test = np.reshape(X_test, (np.shape(X_test['input_1']), 1, np.shape(X_test['input_2'])))\n",
    "preds = np.array([])\n",
    "preds = malstm.predict([X_test['left'], X_test['right']])\n",
    "j = 0\n",
    "correct = 0\n",
    "incorrect = 0\n",
    "preds2 = []\n",
    "for i in preds:\n",
    "  ret = classify(i)\n",
    "  preds2.append(ret)\n",
    "  if ret == test_labels[j]:\n",
    "    correct += 1\n",
    "  else:\n",
    "    incorrect += 1\n",
    "  j+=1\n",
    "#   print(classify(i), test_labels[j])\n",
    "print(correct/ (correct + incorrect))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 52
    },
    "colab_type": "code",
    "id": "nnlJop_WAgIj",
    "outputId": "aa32d367-aea5-4824-dee1-a1ba9bbde19e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "972/972 [==============================] - 1s 715us/step\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0.6188942833569805, 0.6882716049382716]"
      ]
     },
     "execution_count": 31,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "malstm.evaluate([X_test['left'], X_test['right']], test_labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "jvudbANj8E74"
   },
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 139
    },
    "colab_type": "code",
    "id": "ZVu6bhQJBDGr",
    "outputId": "0d3d2a68-5f7b-4740-d2d8-0fa2240d01a6"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "             precision    recall  f1-score   support\n",
      "\n",
      "        0.0       0.69      0.97      0.81       663\n",
      "        1.0       0.57      0.07      0.13       309\n",
      "\n",
      "avg / total       0.66      0.69      0.59       972\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(skm.classification_report(test_labels, preds2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "pRh5N0pGB4Ae"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "main_2.ipynb",
   "provenance": [],
   "version": "0.3.2"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
